{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNPlaygenerator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbPDih6Ijq39IuzGU/aOv+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arturolopm/ML/blob/main/RNNPlaygenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NufvSK0t-KTx",
        "outputId": "0766a534-d8b3-4829-ad4c-66bb512cc0fa"
      },
      "source": [
        "%tensorflow_version 2.x # this line es not required inell you are in a notebook\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x # this line es not required inell you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7nsPMadQbFp"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCjO3FkK-crU"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('MMsatan.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "OzfR6-9TRQdi",
        "outputId": "b9bd0d73-571b-4acd-f017-f86b60091197"
      },
      "source": [
        "from google.colab import files #import nfiles from your pc\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-304631d5-2528-4591-b8e0-5fb74d2fd098\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-304631d5-2528-4591-b8e0-5fb74d2fd098\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Lady Masacre by Mario Mendoza (z-lib.org).epub.txt to Lady Masacre by Mario Mendoza (z-lib.org).epub.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r58qMFlQSF-Q",
        "outputId": "18f2f9a5-7f15-4d4e-f90b-93dcce0ad650"
      },
      "source": [
        "#read, thenmmm decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the numbers of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 373281 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG27XvBVTn4l",
        "outputId": "dd941866-5505-4ff9-ce7d-278d2ad7da14"
      },
      "source": [
        "#Take a look of the forst 150 characters in text\n",
        "print(text[5000:5500])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "que solía dormir en posición fetal y que en varios de mis sueños aparecía mi madre (ya fallecida), siempre bondadosa y cariñosa, acariciándome el pelo, diciéndome cuánto me quería, abrazándome antes de salir para el colegio. Definitivamente la niñez no es un estado superado, sino una dimensión de la conciencia que está ahí, latente, y que se activa por momentos para que nos quede claro que seguimos siendo esos seres diminutos, frágiles y lúdicos que tanto disfrutaban de la protección de sus adul\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iGhvwugUQY5"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "#creating a apmping for unique characters to indeces\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int= text_to_int(text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg1agly6VtOA",
        "outputId": "f809627c-ef31-4f2f-a1a9-2152d9a1439f"
      },
      "source": [
        "#lets look at how part of our text is encoded\n",
        "print(\"text: \", text[:14])\n",
        "print(\"Encoded: \", text_to_int(text[:14]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text:  La vida de Fra\n",
            "Encoded:  [33 48  1 69 56 51 48  1 51 52  1 27 65 48]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nmFzfCHWu6L",
        "outputId": "f3baebaf-de27-4312-e043-24a5722ddc27"
      },
      "source": [
        "#convert nuermic values to text\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:14]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La vida de Fra\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z7jU2wwYlX7"
      },
      "source": [
        "Create training expamples\n",
        "input: hell | output: ello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xows7pFpXgbQ"
      },
      "source": [
        "#create a strea mof chmaramcters from oumr data\n",
        "seq_length = 100 # leght of sequence for a training example\n",
        "expamples_per_epoch =len(text)//(seq_length+1)\n",
        " \n",
        " #create training examples / targets\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhSj6R7oaD42"
      },
      "source": [
        "Next use batch metod mto turn this stream of characters inmto batches of desired length\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTMnmw2VaW1-"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZFMQPImatW0"
      },
      "source": [
        "now use these sequences of length 101 and split them into input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLl8Kbzza3NU"
      },
      "source": [
        "def split_input_target(chunk): # for the example : hello\n",
        "    input_text= chunk[:-1] # hell\n",
        "    target_text = chunk[1:] # ello\n",
        "    return input_text, target_text # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target) # we use map to apply the function avobe to every entry"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OIsYfLodJ5V",
        "outputId": "db53d107-0ad1-41f3-dad2-33d5f91c9494"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "La vida de Frank Molina, periodista especializado en judiciales, da un giro inesperado el día que de\n",
            "\n",
            "OUTPUT\n",
            "a vida de Frank Molina, periodista especializado en judiciales, da un giro inesperado el día que dec\n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "ide abrir una oficina y empezar a trabajar como detective privado. A sus manos llega el caso de un e\n",
            "\n",
            "OUTPUT\n",
            "de abrir una oficina y empezar a trabajar como detective privado. A sus manos llega el caso de un ex\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbqxLznEdBUp"
      },
      "source": [
        "finally make training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VMLdqYicwy-"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) # vocab is numbmer of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "#Buffer size to shuffle the dataset\n",
        "#(TF data is designed to work posibbly with infinite sequences\n",
        "# so it doesn't attempt to shuffñe the entire sequence in memory. Instead\n",
        "# it maintains a buffer inm wich it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R8tTAldgU_N"
      },
      "source": [
        "Build the *model*, we will our use an embedding layer a LTSM and ine dense layer that contains a node for each unique character in our trinig data, the layer will give us a probability distribution over all nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSa0GrhnrCQU",
        "outputId": "8e6d6da4-0d16-4b1e-f225-9ea3f6ec0de5"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                         batch_input_shape=[batch_size, None]),\n",
        "                               tf.keras.layers.LSTM(rnn_units,\n",
        "                                                    return_sequences=True,\n",
        "                                                    stateful=True,\n",
        "                                                    recurrent_initializer='glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model \n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           23296     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 91)            93275     \n",
            "=================================================================\n",
            "Total params: 5,363,547\n",
            "Trainable params: 5,363,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlmUuNP2-tfN"
      },
      "source": [
        "Create loss function\n",
        "but first predictions prior training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7E_2Z-6_vnw",
        "outputId": "f7687ea8-0fc7-4987-f364-1018effbebb0"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch) # ask our model for a prediction on our first batch of training data\n",
        "  print(example_batch_predictions.shape, \"# (batch size, sequence_length, vocab_size)\") #print out the output shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 91) # (batch size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOpeqbiAC2kk",
        "outputId": "9039bdc2-bb05-41ab-f349-5212c551af17"
      },
      "source": [
        "#we can see that the predoction is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-2.1671145e-03 -8.3997729e-05  7.3874439e-03 ...  5.7104309e-03\n",
            "    2.7243162e-03 -2.0176182e-03]\n",
            "  [-8.1098219e-04 -2.1475989e-03  4.7739418e-03 ...  8.9015055e-04\n",
            "    2.8375657e-03 -4.8945681e-03]\n",
            "  [ 1.7900246e-03  6.3053169e-04  2.1636565e-03 ... -1.6084180e-04\n",
            "    1.1616480e-03 -5.9067816e-03]\n",
            "  ...\n",
            "  [ 1.4718919e-03  8.5621709e-03  1.0628840e-03 ...  4.9963454e-03\n",
            "    1.6154278e-02  6.4539639e-03]\n",
            "  [ 3.5397420e-03  5.0443881e-03  4.3546373e-04 ...  1.6358262e-04\n",
            "    1.6398212e-02  1.0683674e-02]\n",
            "  [ 2.1511510e-03  2.5350747e-03 -9.1433985e-04 ...  3.8776253e-03\n",
            "    1.4329702e-02  2.4245176e-03]]\n",
            "\n",
            " [[ 1.1160151e-03 -3.1310152e-03 -3.4986972e-04 ... -5.9874328e-03\n",
            "    2.5330242e-04  9.4693210e-03]\n",
            "  [ 3.2916856e-03 -3.6639879e-03 -2.1014193e-03 ... -8.1586177e-03\n",
            "    5.6619137e-03  1.2093884e-02]\n",
            "  [ 2.4805539e-03 -3.2341997e-03 -2.7923912e-03 ...  1.8480213e-03\n",
            "    1.2666127e-03  4.3316642e-03]\n",
            "  ...\n",
            "  [-1.6188405e-02  8.0116547e-04  4.8368219e-03 ...  1.0398675e-02\n",
            "   -1.5077146e-03 -7.5328024e-03]\n",
            "  [-1.3069805e-02  3.6688156e-03  5.7601240e-03 ...  5.9088310e-03\n",
            "   -6.2679816e-03 -7.9420777e-03]\n",
            "  [-1.0528088e-02  9.6076750e-04  2.9892963e-03 ...  8.1934035e-03\n",
            "   -4.1594841e-03 -1.1266135e-02]]\n",
            "\n",
            " [[ 2.5822090e-03 -1.4609484e-03 -1.2743083e-03 ... -3.5511469e-03\n",
            "    3.9780545e-03  5.5632428e-03]\n",
            "  [ 1.6581627e-03 -2.2768339e-03 -3.0246926e-03 ...  9.5982151e-04\n",
            "    4.4153463e-03 -1.8831994e-03]\n",
            "  [ 4.5710802e-03 -2.5458618e-03 -3.6837191e-03 ... -2.8029075e-03\n",
            "    7.3469467e-03  4.3186201e-03]\n",
            "  ...\n",
            "  [-7.5133191e-04 -3.3261285e-03  3.8352679e-04 ... -2.2537261e-03\n",
            "    1.1087441e-02  7.9387780e-03]\n",
            "  [ 2.7372176e-04 -9.8668784e-04 -4.7330135e-03 ... -7.8065833e-03\n",
            "    1.4893826e-02  9.4484221e-03]\n",
            "  [ 1.2845793e-03  5.8676796e-03 -5.1120976e-03 ... -7.9426728e-03\n",
            "    1.3920769e-02  7.7362163e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.6957925e-04 -1.6085810e-03 -2.2095661e-03 ...  3.3875189e-03\n",
            "    1.0854877e-03 -5.8048014e-03]\n",
            "  [-1.7326269e-03  2.3628939e-03 -2.8299186e-03 ...  2.0326509e-03\n",
            "    3.3095835e-03 -4.1188358e-04]\n",
            "  [-2.4266934e-03  6.3599646e-04 -5.1723169e-03 ...  4.7581848e-03\n",
            "    3.5051806e-03 -7.1571935e-03]\n",
            "  ...\n",
            "  [-4.8737433e-03  4.8878356e-03 -3.5496592e-04 ...  3.1414651e-04\n",
            "    1.5577784e-02  7.6123723e-03]\n",
            "  [-5.2852146e-03  1.7201032e-03  2.5310053e-04 ... -3.0740974e-03\n",
            "    1.4823171e-02  1.9008934e-03]\n",
            "  [-7.5728204e-03  6.9301943e-03 -6.1817151e-03 ... -3.5535684e-03\n",
            "    1.8071774e-02  1.4591869e-03]]\n",
            "\n",
            " [[ 2.3638289e-03  4.2943587e-03 -5.2689165e-03 ... -2.1028246e-03\n",
            "    1.6070256e-02  4.5307394e-04]\n",
            "  [ 1.4473677e-03  2.2138462e-03 -6.2413388e-03 ...  2.2075926e-03\n",
            "    1.4302859e-02 -5.9401565e-03]\n",
            "  [-4.6178512e-04  2.4241721e-03  2.3373475e-03 ...  7.6439101e-03\n",
            "    1.4168089e-02 -6.3895206e-03]\n",
            "  ...\n",
            "  [ 3.2609589e-03  4.6953619e-03  5.9584160e-03 ... -5.0778463e-03\n",
            "    6.3377032e-03  1.0758575e-02]\n",
            "  [ 1.6727606e-03  2.1276097e-03  2.9712752e-03 ... -1.0025664e-03\n",
            "    5.9319581e-03  2.1935801e-03]\n",
            "  [-6.2460359e-04  1.9997803e-03  9.9389236e-03 ...  4.5625875e-03\n",
            "    7.0851073e-03  2.2817584e-04]]\n",
            "\n",
            " [[ 2.5822090e-03 -1.4609484e-03 -1.2743083e-03 ... -3.5511469e-03\n",
            "    3.9780545e-03  5.5632428e-03]\n",
            "  [ 4.4125435e-04  3.1768292e-04 -6.2089122e-04 ...  4.5258976e-03\n",
            "    1.0659674e-03  6.2518134e-03]\n",
            "  [ 2.0544338e-03  6.8535483e-03 -2.7539621e-03 ...  2.7709547e-03\n",
            "    1.5248228e-03  4.6064802e-03]\n",
            "  ...\n",
            "  [ 1.0515024e-02  4.7772708e-03  2.7993296e-03 ... -1.8713530e-03\n",
            "    1.8625798e-02 -6.0652439e-03]\n",
            "  [ 1.0309448e-02  2.0231768e-03  1.1947798e-03 ... -3.9660418e-03\n",
            "    1.8989624e-02  2.4735404e-03]\n",
            "  [ 1.7174786e-02  5.6953146e-03  8.1265485e-03 ... -7.6934155e-03\n",
            "    1.5471357e-02 -1.0218937e-05]]], shape=(64, 100, 91), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oXU_dqLFg7t",
        "outputId": "f6870ed2-5739-43da-bfcd-9910b28efcfe"
      },
      "source": [
        "#lets examine 1 prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-2.1671145e-03 -8.3997729e-05  7.3874439e-03 ...  5.7104309e-03\n",
            "   2.7243162e-03 -2.0176182e-03]\n",
            " [-8.1098219e-04 -2.1475989e-03  4.7739418e-03 ...  8.9015055e-04\n",
            "   2.8375657e-03 -4.8945681e-03]\n",
            " [ 1.7900246e-03  6.3053169e-04  2.1636565e-03 ... -1.6084180e-04\n",
            "   1.1616480e-03 -5.9067816e-03]\n",
            " ...\n",
            " [ 1.4718919e-03  8.5621709e-03  1.0628840e-03 ...  4.9963454e-03\n",
            "   1.6154278e-02  6.4539639e-03]\n",
            " [ 3.5397420e-03  5.0443881e-03  4.3546373e-04 ...  1.6358262e-04\n",
            "   1.6398212e-02  1.0683674e-02]\n",
            " [ 2.1511510e-03  2.5350747e-03 -9.1433985e-04 ...  3.8776253e-03\n",
            "   1.4329702e-02  2.4245176e-03]], shape=(100, 91), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpON9ivRGEku",
        "outputId": "ad6e24eb-73cf-4a09-ec38-fecc0a5c3940"
      },
      "source": [
        "#and finally we'll look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "#it is 65 bc we have 65 characters, each value is the probability of each character ocurring next"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91\n",
            "tf.Tensor(\n",
            "[-2.1671145e-03 -8.3997729e-05  7.3874439e-03  4.7776392e-03\n",
            " -2.1570111e-03 -3.1397783e-03  3.9623759e-04 -5.3829215e-03\n",
            " -1.3799104e-03  2.1133812e-03 -3.0295779e-03  5.3933989e-03\n",
            " -3.3961888e-04  4.4129188e-03  9.4672036e-04 -4.8577201e-04\n",
            "  5.4175453e-04  2.0725611e-03 -3.8496614e-03 -5.5133458e-04\n",
            "  6.2935073e-03 -2.4465765e-03 -1.3631160e-03 -1.5971363e-03\n",
            "  5.2285860e-03 -5.7106209e-03 -1.2696375e-03 -3.1692511e-04\n",
            "  2.9519682e-03 -3.8154968e-03  4.9808377e-04  6.4242573e-05\n",
            " -6.5666466e-04 -1.5951490e-03 -4.4140425e-03 -5.9380592e-03\n",
            " -3.6029774e-03 -3.4411799e-03 -2.1477044e-03  4.5031835e-03\n",
            " -3.1991233e-03  2.8912115e-03 -2.3668626e-04  9.3727442e-04\n",
            "  3.9253281e-03  2.6101095e-03  3.4115498e-03  5.8651953e-03\n",
            "  1.5760981e-04 -2.5889431e-03 -4.7512446e-03 -4.6944933e-04\n",
            " -3.0846084e-03  4.2494875e-03  2.2040070e-03 -2.4355652e-03\n",
            "  5.0942441e-03  2.3762076e-03 -3.6227605e-03  5.4271257e-04\n",
            " -3.4038052e-03  2.7798996e-03  1.7889736e-03 -5.1471209e-03\n",
            " -6.5575317e-03  3.1338884e-03 -2.8974289e-04 -4.7012104e-04\n",
            " -2.9449072e-03 -2.3956085e-04 -6.8821041e-03 -2.3144979e-03\n",
            " -2.1907541e-03  3.3502234e-04 -2.1076226e-04  5.7564350e-04\n",
            "  3.7701647e-03 -1.1791093e-03  2.6432746e-03 -4.3058130e-03\n",
            "  9.5078908e-04  4.6191858e-03 -5.6269700e-03  4.6340167e-05\n",
            "  2.0053938e-03 -2.8656838e-03  1.2900159e-03  2.8564588e-03\n",
            "  5.7104309e-03  2.7243162e-03 -2.0176182e-03], shape=(91,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "TkGYyzjOH9Zy",
        "outputId": "9c09db6d-4d76-4e42-ba92-855fca8943aa"
      },
      "source": [
        "#if we want to determine the preicte character we need to sample the outout distribution (pick a value base on its probability)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "#now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices,(1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars #and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'oRzónET7QcBgI…)9( ÍÉ«0«BJ¿h9?YXaG;«AKdfJbBVivú8-PA…ÁéGx;g0CNüaOADÁiuérCEuPünv?hxjGk 0d)3m!qdqíf)fYÉt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbwywJ-BHIt0"
      },
      "source": [
        "now define a function  to compare output to expected aoutput and give us a numeric value reoresenting how close they are"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAuAzhWIG_Iq"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGFhmOoHMdgB"
      },
      "source": [
        "compiling model\n",
        "innthis point we can think our problem as a classification problemm where the model predicts the probability of each unique letter comming next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzwJsErPNJIz"
      },
      "source": [
        "model.compile(optimizer='adam', loss= loss)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGEcr6XYNUGY"
      },
      "source": [
        "Creating checkpoints \n",
        "now we are configure our odel to save chackpoint as it trains. This will allow is to load our model from a checkpoint and continue training it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptAD0GuHNSVL"
      },
      "source": [
        "#Directory where the checkpoint will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_MM(epoch)\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only= True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB8w_G8hO16t"
      },
      "source": [
        "Training\n",
        "Finally we will start training the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "ouQLcb4GPCLF",
        "outputId": "99bf2f6d-3c06-431c-ed4e-f4c61165c335"
      },
      "source": [
        "history = model.fit(data, epochs =48, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/48\n",
            " 3/57 [>.............................] - ETA: 5:45 - loss: 4.2311"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-81ac9f32fa3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjg9S7nlSwcN"
      },
      "source": [
        "loading the model\n",
        "We'll rebuiñl the model frim a checkpoint using a batch size of 1 so that we can feed one ppiece of text ti the model and hve it make a prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXUibeTJS0r6"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn92ka1JTwOL"
      },
      "source": [
        "once it is finished training we can find the latest checkpoint that stores the models weight using following line.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pQgcJzyUCSc"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMwOTO75UZKw"
      },
      "source": [
        "We can load any checkpoint we want by specifying the exact file to load.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFohkdTjUmzj"
      },
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\"+ str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KivgwKSRVOYJ"
      },
      "source": [
        "Genereating text\n",
        "now we can use thw tensorflow provided function to generate some text any string we'd like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZYJ-rCEVsCU"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  #Evaluation step, generates text using the learned model\n",
        "\n",
        "  #number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # converting our start tring to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  #Empty sting to store the results\n",
        "  text_generated = []\n",
        "  #low temperatures results in more predictable text.\n",
        "  #Higher temperatures results in mmore surrprising text-\n",
        "  #experiment to find the best setting\n",
        "  temperature = 1.0\n",
        "\n",
        "  #here the batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    #remove the batch dimentions\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    #using a categorical distribution to predict the character predicted by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    #We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UH6WrThZfnO",
        "outputId": "a79c53f5-6894-4908-cad6-aa2c0e481f96"
      },
      "source": [
        "inp = input(\"Type a string: \")\n",
        "print(generate_text(model,inp))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a string: Hola, como estas?\n",
            "Hola, como estas?F0Z££îÉ<èêè’V8F£xR~ïÉÉÉêF<èëç~ÉFWëëWF’;~ï4FèèÔwè8x\\(ZçZ<èùèèZ;/'/(à\\ëùèF8Ôë(îûèîîOï4ïW£Zè~Ôè0É;£îF’;'ÉèwÔèè£ÔèZÉZG’’~XëOÎè<ë£î£îÉàZÔ»GGÔ7’è\\FF<ç~îà\\F~È£FÉèàï[ÉêÉèèèï8F\"Z<«0èZR8ZW4'ÂïÉFÔ2Zx<WOàïF~('ZwÔïWç'~F0ïÂèZ'\\0ÉFÔ8ùÈèè/kFêùFë~XZëÈ’Fç<X~(ïFÉÉèîF<OU£ï\\WfF0Z;'F<è~îF9ÈÎ<ÈÂÂî<WÔÉà^«ÎïF£M'RZùLG\\'É»ùw<èFêZçêèô<zî\\/RL£û(ëëW(èèZÂù4ïOOZÈÔ[0J»É;'îwïÉHF7<£ù’~É£ïîZ<WÔà~É;ïZ£à0ïç<'WÔOàèï/FûÈèèîWïOÂ'OïFàÉIèç<ZFà;ÉïO<èèàwïîF0ï0ù/Éè\"ïG’èFèè<£îG6xùëëZïZ'TWùèêZ!Hù'ùàêàZ£''ï^ÉÉèZ<èÔçèè’;8ïÈFÉÎ'àOè\"èX£GÉÉ(Ô~ï£î<Uîè»~îÈç~Zè\\5ÉèîF'àFç»ÉçëÉàèX'6àçxïWÂëZ<'àOà£FEùèë£ïÔéàèèïè\\É4èèÈ~èèèèkÔFÉÈÂïxWèÈ<ZXJÂèàW6èêÉèïHFèÉïW£îF;îGFçW'0F[£’ZFëïë8~[8ïx0Ô£ZRà(êOFêè(ëï’£GFw'ïRîZÉïwX»’'Éù£\\ïîÉ’[ïî<ïÈÉJ8ÈRÔçÉ[èà(’F'ëèZFZÉ£èÔFèR'Oà£èù£OF6<ÉFê(£Â'F)»ïîîèF/F<èèHFçïÈFZÉèUç5ï£GîÉèé£ÈC£ùFFFxFïçFÉçî'à~~ÎùàùMXïù'’FFùçWëZ<à8ZÉZîÉè»àëFÉèÎ''’B[êàGÉçÎÈàôOêÈ:<àà’FÈFÉï8(ëG\"îÉïZ-ëÂÉùFIÉèèèÂï;W0ïùÈèàç£üÈ\\<è<ïÔ:ZFÉèèàPZFïZùJÉÉ/~(îïà0UèÂXù~ZÉ'kÉç(ïW<ï'à0~3<î5Fx8ÔÈXêèFïZëî(FèFà\\~î'ïÔÉÉàùZÉç'àÔFçîÉZÉï'ÈFèwÉÈFÂèèèèkèÔïFFçÉèè\\FGFÈwÔXP0ï~<çFÉ(à\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}